<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="SimLiquid:A Simulation-Based Liquid Perception Pipeline for Robot Liquid Manipulation.">
  <meta name="keywords" content="Simulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Simliquid</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">SimLiquid:A Simulation-Based Liquid Perception Pipeline for Robot Liquid Manipulation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Yan Huang</a><sup>1</sup>,</span>
            <span class="author-block">Jiawei Zhang</a><sup>1</sup>,</span>
            <span class="author-block"><a href="https://www.shoujie.top/">Shoujie. Li</a><sup>1</sup>,</span>
            <span class="author-block"><a href="https://ssr-group.net/">Wenbo Ding</a><sup>1，2</sup>,</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Shenzhen Ubiquitous Data Enabling Key Lab, Shenzhen International Graduate School, Tsinghua University, Shenzhen, China</span>
            <span class="author-block"><sup>2</sup>RISC‐V International Open Source Laboratory, Shenzhen, China</span>
          </div>
          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://doi.org/10.22541/au.173321942.24618438/v1" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span>
                  <span>Paper</span>
                </a>
              </span>
<!--               <span class="link-block">
                <a href="https://doi.org/10.22541/au.173321942.24618438/v1" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <span class="link-block">
                <a href="https://github.com/saprrow/SimLiquid" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-github"></i></span>
                  <span>Code</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Transparent liquid volume estimation is crucial for robot manipulation tasks, such as pouring. 
            However, estimating the volume of transparent liquids is a challenging problem..
          </p>
          <p>
            Most existing methods primarily focus on data collection in the real world,
            and the sensors are fixed to the robot body for liquid volume estimation. 
            These approaches limit both the timeliness of the research process and the flexibility of perception.
            In this paper, we present SimLiquid20k, 
            a high-fidelity synthetic dataset for liquid volume estimation, 
            and propose a YOLO-based multi-modal network trained on fully synthetic data for estimating the volume of transparent liquids. .
          </p>
          <p>
            Extensive experiments demonstrate that our method can effectively transfer from simulation to the real world. 
            In scenarios involving changes in background, viewpoint, and container variations, our approach achieves an average error of 5% in real-world volume estimation. 
            In addition, our work conducts two application experiments integrate with ChatGPT, showcasing the potential of our method in service robotics..
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section> 

</body>
</html>
